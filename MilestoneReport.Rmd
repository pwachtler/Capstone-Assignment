---
title: 'Data Science Capstone: Milestone Report'
author: "Paul Wachtler"
date: "November 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

##Introduction
The purpose of this Milestone Report is to present the progress of my Capstone project.  Specifically, I'll walk through my process for downloading the word data, cleaning it, performing exploratory analysis, and beginning to create my prediction algorithm.

##Loading the Data

After unzipping the data file, I change my working directory to it.  Before I read the data from each file, I read in a list of profanity that I'll use to cleanse my data later.  

Afterwards I open each of the three data files (blogs, news, and tweets), read the lines of the file, then close it out.  Finally I calculate the number of rows in each file along with the file sizes.  I display these numbers as part of my exploratory analysis.
```{r Getting and Loading Data, cache=TRUE}
##Change working directory to obtain english language files
setwd("C:/Users/Paul/Google Drive/Data Science Courses/Capstone/final/en_US")

##Load list of profanity
badWordsURL<-"https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
download.file(badWordsURL,destfile="badWords.txt")
badWords<-read.csv("badWords.txt",header=FALSE,sep="\n",strip.white=TRUE)

##Load Blog Data
blogText<- file("en_US.blogs.txt", open="rb")
blog<- readLines(blogText, encoding="latin1")
close(blogText)
blogRows<-length(blog)

##Load News Data
newsText<- file("en_US.news.txt", open="rb")
news<- readLines(newsText, encoding="latin1")
close(newsText)
newsRows<-length(news)

##Load Twitter Data
twitText<- file("en_US.twitter.txt", open="rb")
twit<- readLines(twitText, encoding="latin1")
close(twitText)
twitRows<-length(twit)


## Calculating file sizes
blogFileSize <- ( file.info("en_US.blogs.txt")$size) / (2^20)
newsFileSize <- ( file.info("en_US.news.txt")$size) / (2^20)
twitterFileSize <- ( file.info("en_US.twitter.txt")$size) / (2^20)

```


Here is a glance at the size and number of lines in each file.

```{r data summary}
	
data.frame(source = c("Blogs", "News", "Twitter"),
           "File Size MB" = c(blogFileSize, newsFileSize, twitterFileSize),
           "Num Lines" = c(blogRows, newsRows, twitRows))

```


## Sampling the Data
Since each file is rather large, I'll take a 1% sample of each.

```{r Data Sampling}

## 1% Data Samples
set.seed(555)
blogSample <- rbinom(length(blog),1,0.1)
blogSample <- blog[ blogSample == 1 ]
newsSample <- rbinom(length(news),1,0.1)
newsSample <- news[ newsSample == 1 ]
twitSample <- rbinom(length(twit),1,0.1)
twitSample<- twit[ twitSample == 1 ]

##Combine the samples into one
combinedText<-paste(blogSample,newsSample,twitSample)
```

##Data Cleanup

To make my prediction algorithm useable, I need to cleanup the data.  I'll start with removing non-ASCII characters.

```{r non-ascii removal}
# find indices of words with non-ASCII characters
nonASCIItext<- grep("combinedText", iconv(combinedText, "latin1", "ASCII", sub="combinedText"))

# subset original vector of words to exclude words with non-ACCII characters
combASCIIonly<- combinedText[ - nonASCIItext]
```

Now I'll convert the data into a corpus for easier processing and continue cleansing it.

```{r data cleanup, cache=TRUE}

##Use the TM package to remove known text patterns

library(tm)
textCorpus<-Corpus(VectorSource(combASCIIonly)) ##Have to convert text to a Corpus to work with the TM package

textCorpus <- tm_map(textCorpus, content_transformer(tolower))
textCorpus <- tm_map(textCorpus, removeNumbers)
textCorpus <- tm_map(textCorpus, removeWords, stopwords("english"))   
textCorpus <- tm_map(textCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
textCorpus <- tm_map(textCorpus, function(x) iconv(x, "latin1", "ASCII", sub=""))
textCorpus <- tm_map(textCorpus, stripWhitespace)


## Remove profanity
textCorpus<-tm_map(textCorpus,removeWords,badWords[,1])


# Convert data to plain text
textCorpus <- tm_map(textCorpus, PlainTextDocument)

```


##n-Gram Analysis
Now I'll use n-gram analysis to determine the frequency of words.  I'll create different n-grams from the textCorpus field.  Afterwards I'll construct term-document matrices for each of the n-grams.  These functions come from the RWeka library.

```{r n-grams, cache=TRUE}
#Word/phrase count function
freqDataFrame <- function(tdm){
  # Helper function to tabulate frequency
  freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
  freqDataFrame <- data.frame(word=names(freq), freq=freq)
  return(freqDataFrame)
}


#n-gram Functions
library(RWeka)
library(ggplot2)
singleFunc<-function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
biFunc<-  function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
triFunc<-  function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
quadFunc<-  function(x) unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE)


#Constructing the nGrams
singles<- TermDocumentMatrix(textCorpus, control=list(tokenize=singleFunc))
singles <- removeSparseTerms(singles, 0.99)
singlefreq <- freqDataFrame(singles)


#single words
singlefreq$word <- factor(singlefreq$word, levels = singlefreq$word[order(-singlefreq$freq)])
g <- ggplot(subset(singlefreq,freq>20000), aes(word, freq))    
g <- g + geom_bar(stat="identity", fill="navy")
g <- g + xlab("Words")+ylab("Frequency")
g <- g + theme(axis.text.x=element_text(angle=45, hjust=1))   
g <- g + ggtitle("Plot of Most Frequent Words.")
g 

#bigrams
biGram<- TermDocumentMatrix(textCorpus, control=list(tokenize=biFunc))
biGram <- removeSparseTerms(biGram, 0.999)
biGramfreq <- freqDataFrame(biGram)

biGramfreq$word <- factor(biGramfreq$word, levels = biGramfreq$word[order(-biGramfreq$freq)])
g <- ggplot(subset(biGramfreq,freq>1000), aes(word, freq))    
g <- g + geom_bar(stat="identity", fill="orange2")
g <- g + xlab("Bigrams")+ylab("Frequency")
g <- g + theme(axis.text.x=element_text(angle=45, hjust=1))   
g <- g + ggtitle("Plot of Most Frequent Bigrams.")
g 

#trigrams
triGram<- TermDocumentMatrix(textCorpus, control=list(tokenize=triFunc))
triGram <- removeSparseTerms(triGram, 0.999)
triGramfreq <- freqDataFrame(triGram)

triGramfreq$word <- factor(triGramfreq$word, levels = triGramfreq$word[order(-triGramfreq$freq)])
g <- ggplot(triGramfreq, aes(word, freq))    
g <- g + geom_bar(stat="identity", fill="green4")
g <- g + xlab("Trigrams")+ylab("Frequency")
g <- g + theme(axis.text.x=element_text(angle=45, hjust=1))   
g <- g + ggtitle("Plot of Most Frequent Trigrams.")
g 

#quadgrams
quadGram<- TermDocumentMatrix(textCorpus, control=list(tokenize=quadFunc))
quadGram <- removeSparseTerms(quadGram, 0.9999)
quadGramfreq <- freqDataFrame(quadGram)

quadGramfreq$word <- factor(quadGramfreq$word, levels = quadGramfreq$word[order(-quadGramfreq$freq)])
g <- ggplot(subset(quadGramfreq,freq>40), aes(word, freq))       
g <- g + geom_bar(stat="identity", fill="firebrick4")
g <- g + xlab("Quadgrams")+ylab("Frequency")
g <- g + theme(axis.text.x=element_text(angle=45, hjust=1))   
g <- g + ggtitle("Plot of Most Frequent Quadgrams.")
g 

```

##Next Steps
Now that I'm able to calculate n-grams from a sampling of words, I'll be able to use a back off technique with the n-grams to figure out the next word or series of words.  This means that when a word is given, to find the next word or words, I'll start with the quadgram.  If no quadgram is found, I'll back off to the trigrams then to the bigrams.

This method will be used to build my prediction algorithm, which I'll then build into a Shiny App.